{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this i am going to tell you how to do the k-means clustering\n",
    "# so before we do clustering there are several prerequisites.\n",
    "# Notice that here we use Gensim to make our vectors and cluster.\n",
    "\n",
    "# the k-means algorithm basically takes in data points as vectors and groups the similar data points into clusters\n",
    "# the clustered elemnts depend on the no. of clusters.\n",
    "# so here we are unaware of the no. of clusters\n",
    "\n",
    "\n",
    "# lets divide this into various modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this module we are only going to get the imports, now I have clearly explained why we need these imports and \n",
    "# their contribution to the program.\n",
    "\n",
    "import nltk    # meet nltk which is natural language tool kit, it allows us to work with NLP.\n",
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here We are going to see what are the checkpoints we need to complete inorder to get a good cluster:\n",
    "# 1. pre processing - this involves removing unwanted words(known as stop wprds), symbols and no.'s in some cases\n",
    "# 2. tokenizing - here we make our entire text document to collection of sentences further into collection of words\n",
    "# 3. Making Bag of words and finding the frequency of each words - now let's explore this deply.\n",
    "        # Making BOW - what is Bag of Words, it literally means making the document into collection of documents.\n",
    "        # the BOW is further used to gnerate a dictionary which i'll talk in next point. This BOW is the collection\n",
    "        # of all words in our corpus. There are reallya lot of ways to make BOW it depends on you though.\n",
    "        # FREQUENCY - it is simply the frequency of words in our corpus.\n",
    "# 4. There is a lot of things we can do once we formed the BOW, we could possibly:\n",
    "        # 1. Make a dictionary out of the bow corpus which can be used to store all the words with a unique token id\n",
    "             # for each of them.\n",
    "        # 2. Use this to make a BOW vector, the way we form vectors can also be of two types, which i'll go into \n",
    "             # detail in the upcoming section.\n",
    "#5. Building a Vector representation of the corpus. - Now this is very important as we cannot actually manipulate \n",
    "        # the text data but we can actually make use of vectors, which can be added, made dot and cross product\n",
    "        #, found distance between two vectors, etc. So each vector should ideally represent the document or the \n",
    "        # sentence id, and the all the terms in the dictionary by this way we can actually build a matrix, which\n",
    "        # is the next step that is converting this vector into a TFIDF Matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
